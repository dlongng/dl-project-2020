{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BVgwiRKDoz7",
        "outputId": "b3971211-e075-43e6-af0b-52591e113c92"
      },
      "source": [
        "from keras import layers\r\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.utils import layer_utils\r\n",
        "from keras.utils.data_utils import get_file\r\n",
        "from keras.applications.imagenet_utils import preprocess_input\r\n",
        "import pydot\r\n",
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "from keras.utils import plot_model\r\n",
        "\r\n",
        "from keras.initializers import glorot_uniform\r\n",
        "import scipy.misc\r\n",
        "from matplotlib.pyplot import imshow\r\n",
        "%matplotlib inline\r\n",
        "import os\r\n",
        "\r\n",
        "from functools import partial\r\n",
        "\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Concatenate\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Lambda\r\n",
        "from tensorflow.keras.layers import MaxPooling2D\r\n",
        "from tensorflow.keras.layers import add\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "K.set_image_data_format('channels_last')\r\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-7e0081563143>:35: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
            "Instructions for updating:\n",
            "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0F3u3WDoPtl",
        "outputId": "d05e9db9-a01c-4d6a-a642-d2ba08180149"
      },
      "source": [
        "import tensorflow\r\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_ChM6mWlXuW",
        "outputId": "41f362d8-83a4-4574-dbd3-4eecf2b75c7b"
      },
      "source": [
        "!pip install face-alignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face-alignment\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/5d/2a92bde2e2cc5b695f45901911e7dc5ed4603ec50ca532fc6398d5706990/face_alignment-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.4.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from face-alignment) (0.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from face-alignment) (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from face-alignment) (4.1.2.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.7.0+cu101)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (2.5)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (7.0.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (3.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->face-alignment) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->face-alignment) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->face-alignment) (0.8)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->face-alignment) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.15.0)\n",
            "Installing collected packages: face-alignment\n",
            "Successfully installed face-alignment-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZPU24vvlXxW"
      },
      "source": [
        "import face_alignment\r\n",
        "\r\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, face_detector='sfd')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sCkqnPXwXQe",
        "outputId": "1a5bf87f-419d-4755-b78b-26c8fed72a01"
      },
      "source": [
        "fa.face_detector.reference_scale"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAnzneWDvQgU"
      },
      "source": [
        "detected_faces = fa.face_detector.detect_from_image(input[..., ::-1].copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNr_q3LLvavP",
        "outputId": "7de1aa18-9cc2-4587-9e08-556b71ebeb51"
      },
      "source": [
        "detected_faces[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([372.92505, 136.7204 , 746.49475, 660.4481 ,   1.     ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR_YcQuLlX0L"
      },
      "source": [
        "from skimage import io\r\n",
        "input = io.imread('/content/Capture.PNG')\r\n",
        "input = input[:,:,:3]\r\n",
        "res = fa.get_landmarks(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYC0Mnjmq46L",
        "outputId": "d131dca3-5532-4d17-833f-4a96e101b38e"
      },
      "source": [
        "len(res[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaQ2BiSKDzTr",
        "outputId": "f59cb2d1-c90f-4777-d28d-43b698bb48fa"
      },
      "source": [
        "!pip install keras-facenet\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-facenet\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/c5/6fadf919a86c44b87ba9d8134cc83820b8fa8a98f5c68ff676179e052839/keras-facenet-0.3.2.tar.gz\n",
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn->keras-facenet) (2.4.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn->keras-facenet) (4.1.2.30)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras-facenet) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras-facenet) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras-facenet) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn->keras-facenet) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn->keras-facenet) (1.15.0)\n",
            "Building wheels for collected packages: keras-facenet\n",
            "  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-cp36-none-any.whl size=10387 sha256=26483e558bfeb974c6d5a2e1c0b6d2ce93e5322adb644e7a840fc28968e64361\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/53/9a/36c4b52fd22faf4f710d5047d874655b85a1b2cf77accfb9bd\n",
            "Successfully built keras-facenet\n",
            "Installing collected packages: mtcnn, keras-facenet\n",
            "Successfully installed keras-facenet-0.3.2 mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cQA6clID1TD"
      },
      "source": [
        "from keras_facenet import FaceNet\r\n",
        "embedder = FaceNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfzegl3SD6H1"
      },
      "source": [
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "import tensorflow as tf\r\n",
        "initializer = tf.keras.initializers.HeUniform()\r\n",
        "inputs = Input(shape=(160, 160, 3))\r\n",
        "out = embedder.model(inputs)\r\n",
        "X = Flatten()(out)\r\n",
        "# X = Dense(256,  kernel_initializer = glorot_uniform(seed=0),kernel_regularizer=regularizers.l2(1e-4))(X)\r\n",
        "# X = BatchNormalization()(X)\r\n",
        "# X = Activation('relu')(X)\r\n",
        "X = Dropout(0.2)(X)\r\n",
        "out = Dense(7, activation='softmax', name='fc7', kernel_initializer=initializer )(X)\r\n",
        "model = Model(inputs, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MKWhxb8D84x",
        "outputId": "b3f86b90-3b3a-40dc-c71c-e3d0b7b2e48a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v1 (Functio (None, 512)               23497424  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc7 (Dense)                  (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 23,501,015\n",
            "Trainable params: 23,471,415\n",
            "Non-trainable params: 29,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R7DyzjEEPU-"
      },
      "source": [
        "model_path = \"/content/drive/MyDrive/btl_dl/final_dl/facenet_0.69.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XCLv-dbFWOf"
      },
      "source": [
        "for layer in model.layers[1].layers[:-25] :\r\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IuYpUvPEB1q"
      },
      "source": [
        "model.load_weights(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBtkth3L9lF1",
        "outputId": "3f7b6eb8-14a3-4d7c-8c36-816b4a00d086"
      },
      "source": [
        "model = None\r\n",
        "from tensorflow.keras.applications import EfficientNetB2\r\n",
        "eff = EfficientNetB2(include_top = False, input_shape=(96, 96, 3), pooling='avg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
            "31793152/31790344 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFq-bM_B9lSb"
      },
      "source": [
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "\r\n",
        "inputs = Input(shape=(96, 96, 3))\r\n",
        "out = eff(inputs)\r\n",
        "X = Flatten()(out)\r\n",
        "X = Dropout(0.2)(X)\r\n",
        "# X = Dense(32,  kernel_initializer = glorot_uniform(seed=0),kernel_regularizer=regularizers.l2(1e-4))(X)\r\n",
        "# X = BatchNormalization()(X)\r\n",
        "# X = Activation('relu')(X)\r\n",
        "# X = Dropout(0.2)(X)\r\n",
        "out = Dense(7, activation='softmax', name='fc7', kernel_initializer = glorot_uniform(seed=0))(X)\r\n",
        "model = Model(inputs, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbfD9h359lVk"
      },
      "source": [
        "model_path = \"/content/drive/MyDrive/btl_dl/final_dl/eff.hdf5\"\r\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__AWrf7O99hF"
      },
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP8Qw1DAEWv4"
      },
      "source": [
        "# from google.colab import files\r\n",
        "\r\n",
        "# uploaded = files.upload()\r\n",
        "\r\n",
        "# for fn in uploaded.keys():\r\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\r\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EakK8r1GaG6"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "mtcnn = FaceNet()\r\n",
        "# 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\r\n",
        "index2label = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'sad', 5: 'surprise', 6: 'neutral'}\r\n",
        "\r\n",
        "# Create a VideoCapture object and read from input file\r\n",
        "# If the input is the camera, pass 0 instead of the video file name\r\n",
        "cap = cv2.VideoCapture('/content/WIN_20201212_10_59_40_Pro.mp4')\r\n",
        "frame_width = int(cap.get(3))\r\n",
        "frame_height = int(cap.get(4))\r\n",
        "cap.set(cv2.CAP_PROP_FPS, 30)\r\n",
        "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\r\n",
        "out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\r\n",
        "# Check if camera opened successfully\r\n",
        "if (cap.isOpened()== False): \r\n",
        "  print(\"Error opening video stream or file\")\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# Read until video is completed\r\n",
        "while(cap.isOpened()):\r\n",
        "  # Capture frame-by-frame\r\n",
        "  ret, frame = cap.read()\r\n",
        "  if ret == True:\r\n",
        "\r\n",
        "    # Display the resulting frame\r\n",
        "    # cv2.imshow('Frame',frame)\r\n",
        "    image = cv2.flip(frame,1)\r\n",
        "    # detections, imgs = mtcnn.crop(image, threshold=0.5)\r\n",
        "    detected_faces = fa.face_detector.detect_from_image(image[..., ::-1].copy())\r\n",
        "    # if len(imgs) > 0:\r\n",
        "      # detections = sorted(enumerate(detections), key = lambda x : x[1]['confidence'], reverse = True)\r\n",
        "    if len(detected_faces) >0 :\r\n",
        "      detected_faces = sorted(enumerate(detected_faces), key = lambda x : x[1][-1], reverse = True)[0][1]\r\n",
        "      bb = detected_faces[:-1]\r\n",
        "      # bb = detections[0][1]['box']\r\n",
        "      # idx = detections[0][0]\r\n",
        "      # img = imgs[idx]\r\n",
        "      w = bb[2]-bb[0]\r\n",
        "      h = bb[3]-bb[1]\r\n",
        "      height,width, _ = image.shape\r\n",
        "      img = image[max(0,int(bb[1]-h*0.15)): int(min(bb[3]+h*0.15,height)), int(max(0,bb[0]-w*0.15)): int(min(bb[2]+w*0.15,width)), :]\r\n",
        "      # img = cv2.resize(img,(160,160))\r\n",
        "      img = cv2.resize(img,(96,96))\r\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \r\n",
        "      # nor_img = (gray - 127.5) / 127.5\r\n",
        "      nor_img = gray\r\n",
        "      nor_img = np.stack((nor_img,nor_img,nor_img)).transpose(1,2,0)\r\n",
        "      nor_img = preprocess_input(nor_img)\r\n",
        "      res = model.predict(np.array([nor_img]))\r\n",
        "      idx = np.argmax(res[0])\r\n",
        "      emo = index2label[idx]\r\n",
        "      x1, y1, x2, y2 = bb\r\n",
        "      # x1, y1, w, h = bb\r\n",
        "      # x2, y2 = x1+w, y1+h\r\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0),2)\r\n",
        "      front_scale=(w + h)/130\r\n",
        "      thickness =int(front_scale / 4)+1\r\n",
        "      if thickness < 1 :\r\n",
        "          thickness=1\r\n",
        "      if front_scale <1:\r\n",
        "          front_scale=1\r\n",
        "      cv2.putText(image, emo, (x1,y2), cv2.FONT_HERSHEY_PLAIN,front_scale,(255,255,255),thickness,cv2.LINE_AA)\r\n",
        "      # cv2_imshow(image)\r\n",
        "      cv2.imwrite(\"now.jpg\",image)\r\n",
        "    out.write(image)\r\n",
        "\r\n",
        "    # if len(detections) != 0 :\r\n",
        "    #   detections = sorted(test, key = lambda x : x['confidence'], reverse = True)\r\n",
        "    #   res = detections[0]\r\n",
        "\r\n",
        "    # print(detections)\r\n",
        "    # break\r\n",
        "    # Press Q on keyboard to  exit\r\n",
        "    # if cv2.waitKey(25) & 0xFF == ord('q'):\r\n",
        "    #   break\r\n",
        "\r\n",
        "  # Break the loop\r\n",
        "  else: \r\n",
        "    break\r\n",
        "\r\n",
        "# When everything done, release the video capture object\r\n",
        "cap.release()\r\n",
        "out.release()\r\n",
        "\r\n",
        "# Closes all the frames\r\n",
        "cv2.destroyAllWindows()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpZ5p9xRKBDM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}